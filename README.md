<p align="center">
  <img src="assets/banner.png" alt="Memex â€” Your AI-powered knowledge companion for Obsidian" width="100%" />
</p>

<p align="center">
  <strong>Auto-tag notes Â· Summarise your week Â· Extract action items Â· Chat with your vault</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/version-1.0.0-blue?style=flat-square" alt="Version" />
  <img src="https://img.shields.io/badge/license-MIT-green?style=flat-square" alt="License" />
  <img src="https://img.shields.io/badge/AI-Local%20%7C%20Gemini-orange?style=flat-square" alt="AI Providers" />
</p>

---

## âš¡ Quick Start â€” up and running in 60 seconds

```bash
# 1ï¸âƒ£  Build the plugin
npm install && npm run build

# 2ï¸âƒ£  Install into your vault
mkdir -p <your-vault>/.obsidian/plugins/memex
cp main.js manifest.json <your-vault>/.obsidian/plugins/memex/

# 3ï¸âƒ£  Enable in Obsidian
#     Settings â†’ Community Plugins â†’ Reload â†’ Toggle "Memex" ON
#     Then set your AI provider (Local or Gemini) in Settings â†’ Memex
```

> **Using Gemini?** Grab a free API key from [Google AI Studio](https://aistudio.google.com/apikey) â€” no local server needed.
>
> **Using a local LLM?** Start [LM Studio](https://lmstudio.ai) or [Ollama](https://ollama.com) and load a chat + embedding model.

---

## âœ¨ Features

### ğŸ’¬ Chat with Your Vault
- **Streaming responses** â€” tokens appear in real-time as the LLM generates them, with automatic fallback to non-streaming if the server doesn't support it.
- **Multiple conversations** â€” create, rename, and delete chats; sidebar lists them sorted by last activity.
- **Resizable sidebar** â€” drag the divider or toggle the sidebar open/closed.
- **Message actions** (per-message `â‹¯` menu):
  - **Copy** any message to clipboard.
  - **Edit & re-submit** a user message (trims history and re-generates).
  - **Regenerate** an assistant response.
  - **Delete** a single message.
  - **Export to Note** â€” saves a message as a new Markdown file in your vault.
- **Per-conversation settings** â€” override temperature, max tokens, system prompt, RAG top-K, and similarity threshold on a per-chat basis via the âš™ï¸ button.
- **Personas** â€” switch the assistant's personality (e.g., *Zettelkasten Guide*, *Daily Reflector*, *Concise Summarizer*). Fully customisable in settings.
- **PDF export** â€” right-click a chat in the sidebar â†’ *Export to PDF*. Renders full Markdown with styled headings, code blocks, and lists into an A4 PDF saved to `Memex/PDFs/`.

### ğŸ” RAG (Retrieval-Augmented Generation)
- **Intelligent query rewriting** â€” follow-up questions are automatically rewritten into standalone search queries using the LLM, so context isn't lost across turns.
- **Content-hash indexing** â€” only re-embeds notes whose content has actually changed; hashes are persisted to disk across reloads, eliminating redundant API calls.
- **Idle-based auto-indexing** â€” dirty files are queued and re-indexed when you navigate away from a note (not on every keystroke).
- **Optimised vector store**:
  - `Float32Array` embeddings with pre-computed norms for ~2â€“3Ã— faster cosine similarity.
  - Min-heap top-K search â€” avoids sorting the entire index.
  - Path index for O(1) document lookups and deletions.
  - Compact JSON serialisation (~40% smaller on disk).
- **Excluded folders** â€” keep `Templates`, `.obsidian`, or any other folders out of the index.
- **Manual & automatic** â€” index the full vault on demand, or let the watcher handle it.

### ğŸ·ï¸ Note Processing Commands
- **Auto Tag Current Note** â€” LLM suggests 3â€“5 relevant tags and prepends them.
- **Extract Action Items** â€” finds TODOs/tasks and appends them as a checklist.
- **Generate Weekly Summary** â€” summarises all notes modified in the last 7 days and saves the result to `Weekly Summaries/{Year}/`.

### ğŸ”Œ Modular AI Provider System
Swap between providers at any time â€” no restart required:

| | Local (LM Studio / Ollama) | Google Gemini |
|---|---|---|
| **Chat model** | `qwen/qwen3-vl-4b` (default) | `gemini-2.5-flash` (default) |
| **Embedding model** | `text-embedding-nomic-embed-text-v1.5` | `gemini-embedding-001` |
| **Server** | Your local machine | Google Cloud (API key) |

Both providers implement the same `ILLMProvider` / `IEmbeddingProvider` interfaces using OpenAI-compatible endpoints, so any model that speaks that protocol works.

---

## ğŸ® Usage

### Commands
Open the Command Palette (`Cmd/Ctrl + P`) and search for **Memex**:

| Command | Description |
|---------|-------------|
| **Open Chat with Journal** | Opens the chat sidebar |
| **Auto Tag Current Note** | Analyses the note and prepends tags |
| **Extract Action Items** | Finds TODOs and appends a checklist |
| **Generate Weekly Summary** | Summarises the last 7 days of notes |
| **Index Vault for RAG** | Full vault embedding index (with progress) |
| **Clear RAG Index** | Wipes the index for a fresh rebuild |
| **View RAG Index Statistics** | Shows total indexed document chunks |
| **Debug RAG Retrieval** | Select text â†’ retrieves matching chunks (logged to console) |

### Chat Interface
- Click the **ğŸ’¬ ribbon icon** or run the *Open Chat with Journal* command.
- Type a message and press **Enter** (or **Shift+Enter** for a new line).
- Use the **âš™ï¸** button next to Send to adjust per-chat settings.
- Right-click a conversation in the sidebar for rename / export / delete options.

---

## âš™ï¸ Settings

Go to **Settings â†’ Memex**.

### AI Provider
- **Provider**: Local (LM Studio / Ollama) or Google Gemini.

### Local LLM Settings
- **LLM Endpoint** â€” URL of your local server (default `http://localhost:1234`).
- **Chat Model** â€” model identifier for chat completions.
- **Embedding Model** â€” model identifier for embeddings.

### Google Gemini Settings
- **API Key** â€” your Gemini API key.
- **Chat Model** â€” Gemini model (default `gemini-2.5-flash`).
- **Embedding Model** â€” Gemini embedding model (default `gemini-embedding-001`).

### General
- **Weekly Summary Path** â€” folder for weekly summaries.
- **Default Temperature** â€” controls randomness (0.0â€“1.0).
- **Default Max Tokens** â€” maximum response length.

### RAG Settings
- **Enable RAG** â€” toggle retrieval features on/off.
- **Chunk Size** â€” words per chunk (default 200).
- **Chunk Overlap** â€” overlapping words between chunks (default 30).
- **Top K Results** â€” number of chunks to retrieve (default 6).
- **Similarity Threshold** â€” minimum relevance score (default 0.4).
- **Auto-Index on Change** â€” re-index notes on create/modify/delete.
- **Excluded Folders** â€” comma-separated list of folders to skip.

### Personas
- **Personas JSON** â€” edit the array of `{name, prompt}` objects to customise assistant behaviour.

---

## ğŸ—ï¸ Architecture

```
main.ts                  â†’ Plugin entry point, settings, commands
â”œâ”€â”€ providers.ts         â†’ LLM & Embedding provider interfaces + Local/Gemini implementations
â”œâ”€â”€ llm_service.ts       â†’ LLM service (completion + streaming)
â”œâ”€â”€ embedding_service.ts â†’ Chunking + batch embedding generation
â”œâ”€â”€ vector_store.ts      â†’ JSON-backed vector store with Float32Array + min-heap search
â”œâ”€â”€ rag_service.ts       â†’ RAG orchestration (indexing, retrieval, query rewriting)
â”œâ”€â”€ processor.ts         â†’ Note processing (tags, action items, weekly summary)
â”œâ”€â”€ conversation_manager.ts â†’ Conversation CRUD (JSON files in .memex/)
â””â”€â”€ chat_view.ts         â†’ Chat UI (sidebar, messages, streaming, PDF export)
```

---

## ğŸ—ºï¸ Roadmap

- [ ] **Multi-modal notes** â€” image and PDF understanding via vision models
- [ ] **Graph-aware RAG** â€” leverage Obsidian's link graph to boost retrieval relevance
- [ ] **Ollama auto-detect** â€” automatically discover running models, no manual config
- [ ] **Mobile support** â€” optimise the chat UI and indexing for Obsidian Mobile
- [ ] **Semantic search command** â€” vault-wide natural language search from the command palette
- [ ] **Note generation** â€” create new notes from chat responses with backlinks
- [ ] **Scheduled summaries** â€” automatic daily/weekly/monthly summaries on a cron
- [ ] **Plugin marketplace** â€” submit to the Obsidian Community Plugins directory

Have an idea? [Open an issue](https://github.com/manas-33/memex/issues) â€” PRs welcome!

---

## ğŸ¤ Contributing

Contributions are welcome â€” whether it's a bug fix, new feature, or documentation improvement.

1. **Fork** the repo and create a new branch:
   ```bash
   git checkout -b feature/my-feature
   ```
2. **Make your changes** â€” follow the existing code style and add comments where needed.
3. **Build & test** to make sure everything compiles:
   ```bash
   npm run build
   ```
4. **Submit a Pull Request** with a clear description of what you changed and why.

### Development Setup

```bash
git clone https://github.com/manas-33/memex.git
cd memex
npm install
npm run dev    # Watch mode â€” rebuilds on file changes
```

Then symlink or copy the built files into your vault's `.obsidian/plugins/memex/` folder and reload Obsidian.

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE) for details.
